import os
import random
import numpy as np
import json
import time
import sqlite3
from collections import Counter, deque
from io import BytesIO
import matplotlib.pyplot as plt
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    ContextTypes,
    CallbackQueryHandler,
)
from telegram.constants import ParseMode
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.calibration import CalibratedClassifierCV
from typing import List, Dict, Tuple, Optional, Any

# --- Constants and Environment Variables ---
TOKEN = os.getenv("TELEGRAM_TOKEN")
if not TOKEN:
    raise ValueError("TELEGRAM_TOKEN environment variable not found!")

BOT_NAME = "üëë Bot T√†i X·ªâu Pro üëë"
DATABASE_NAME = "tx_feedback.db"
DATA_PERSISTENT_PATH = "bot_data.json"
HISTORY_DATA_PATH = "cau.json"

# --- Data Structures ---
history_data: deque = deque(maxlen=600)
train_data: List[List[str]] = []
train_labels: List[str] = []
user_feedback_history: deque = deque(maxlen=1000)
sentimental_analysis: Dict[str, Any] = {}
last_prediction: Dict[str, Optional[str]] = {
    "result": None,
    "strategy": None,
    "model": None,
}

# --- ML Models and Setup ---
le = LabelEncoder()
scaler = StandardScaler()
poly = PolynomialFeatures(degree=2, include_bias=False)

model_logistic = LogisticRegression(random_state=42, solver="liblinear", C=1.1, penalty="l1")
model_svm = SVC(kernel="linear", probability=True, random_state=42, C=1.4)
model_sgd = SGDClassifier(loss="log_loss", random_state=42, alpha=0.01)
model_rf = RandomForestClassifier(
    random_state=42, n_estimators=150, max_depth=9, min_samples_split=2
)
model_gb = GradientBoostingClassifier(
    n_estimators=110, learning_rate=0.1, max_depth=6, random_state=42, subsample=0.9
)
model_nb = GaussianNB()
model_calibrated_svm = CalibratedClassifierCV(model_svm, method="isotonic", cv=5)
model_kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)

models_to_calibrate = [model_logistic, model_sgd, model_rf, model_gb]
calibrated_models: Dict[Any, Any] = {}

# --- Weights for Strategy Adjustments ---
feedback_weights = {"correct": 1.3, "incorrect": -0.90}
strategy_weights = {
    "deterministic": 0.75,
    "cluster": 0.70,
    "machine_learning": 1.55,
    "probability": 0.50,
    "streak": 0.55,
    "statistical": 0.30,
    "boosting": 1.25,
    "mathematical": 0.65,
    "statistical_algo": 0.45,
    "statistical_analysis": 0.40,
    "numerical_analysis": 0.35,
    "search_algorithm": 0.55,
    "automatic_program": 0.75,
    "theorem_algo": 0.60,
    "evolutionary_algo": 0.95,
    "reading_opportunity": 0.35,
}

# --- Database Functions ---
def create_feedback_table() -> None:
    """Creates the user feedback table in the database if it doesn't exist."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS user_feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            feedback_type TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    """
    )
    conn.commit()
    conn.close()


def save_user_feedback(feedback: str) -> None:
    """Saves user feedback to the database."""
    conn = sqlite3.connect(DATABASE_NAME)
    cursor = conn.cursor()
    try:
        cursor.execute(
            "INSERT INTO user_feedback (feedback_type) VALUES (?)", (feedback,)
        )
        conn.commit()
    except sqlite3.Error as e:
        print(f"Database exception during user feedback saving: {e}")
    finally:
        conn.close()

# --- Data Loading/Saving Functions ---
def load_cau_data() -> None:
    """Loads historical game data from a JSON file."""
    global history_data
    if os.path.exists(HISTORY_DATA_PATH):
        try:
            with open(HISTORY_DATA_PATH, "r") as f:
                loaded_data = json.load(f)
                history_data = deque(loaded_data, maxlen=600)
        except json.JSONDecodeError as e:
            print(f"Error decoding cau data JSON: {e}, file may be empty or corrupted. Creating new file.")
            history_data=deque(maxlen=600)
            save_cau_data()
        except Exception as e:
            print(f"Error loading cau data: {e}")


def save_cau_data() -> None:
    """Saves historical game data to a JSON file."""
    global history_data
    try:
        with open(HISTORY_DATA_PATH, "w") as f:
            json.dump(list(history_data), f)
            print("Cau data saved to file.")
    except Exception as e:
        print(f"Error saving cau data: {e}")


def load_data_state() -> None:
    """Loads bot state data from a JSON file."""
    global strategy_weights, last_prediction, user_feedback_history, history_data, calibrated_models, train_data, train_labels, sentimental_analysis
    if os.path.exists(DATA_PERSISTENT_PATH):
        try:
            with open(DATA_PERSISTENT_PATH, "r") as f:
                loaded_data = json.load(f)
                strategy_weights = loaded_data.get("strategy_weights", strategy_weights)
                last_prediction = loaded_data.get("last_prediction", last_prediction)
                user_feedback_history = deque(
                    loaded_data.get("user_feedback_history", []), maxlen=1000
                )
                history_data = deque(loaded_data.get("history_data", []), maxlen=600)
                train_data = loaded_data.get("train_data", [])
                train_labels = loaded_data.get("train_labels", [])
                calibrated_models = loaded_data.get(
                    "calibrated_models", calibrated_models
                )
                sentimental_analysis = loaded_data.get(
                    "sentimental_analysis", sentimental_analysis
                )
                print("Bot data state loaded from file.")
        except json.JSONDecodeError as e:
            print(f"Error decoding data state JSON: {e}, file may be empty or corrupted. Creating new file.")
            save_data_state()
        except Exception as e:
             print(f"Error loading data state: {e}")

def save_data_state() -> None:
    """Saves the bot's state data to a JSON file."""
    global strategy_weights, last_prediction, user_feedback_history, history_data, calibrated_models, train_data, train_labels, sentimental_analysis
    try:
        with open(DATA_PERSISTENT_PATH, "w") as f:
             json.dump({
                "strategy_weights": strategy_weights,
                "last_prediction": last_prediction,
                "user_feedback_history": list(user_feedback_history),
                "history_data": list(history_data),
                 "train_data": train_data,
                 "train_labels": train_labels,
                 "calibrated_models": {str(k): v.__dict__ for k, v in calibrated_models.items()} ,
                 "sentimental_analysis": sentimental_analysis
             }, f)
        print("Bot data state saved to file.")
    except Exception as e:
        print(f"Error saving data file: {e}")

# --- Chart Generation ---
def generate_history_chart(history: List[str]) -> Optional[BytesIO]:
    """Generates a bar chart of the recent game history."""
    if not history:
        return None
    labels, values = zip(*Counter(history).items())
    plt.figure(figsize=(8, 5))
    plt.bar(labels, values, color=["skyblue", "salmon"])
    for i, v in enumerate(values):
        plt.text(labels[i], v + 0.1, str(v), ha="center", va="bottom")
    plt.xlabel("üé≤ K·∫øt qu·∫£ (T: T√†i, X: X·ªâu)", fontsize=12)
    plt.ylabel("üìä T·∫ßn su·∫•t", fontsize=12)
    plt.title("üìà Ph√¢n B·ªë K·∫øt Qu·∫£ G·∫ßn Nh·∫•t", fontsize=14)
    buffer = BytesIO()
    plt.savefig(buffer, format="png")
    buffer.seek(0)
    plt.close()
    return buffer


# --- Prediction Logic ---
def calculate_probabilities(history: List[str]) -> Dict[str, float]:
    """Calculates probabilities for 't' and 'x'."""
    if not history:
        return {"t": 0.5, "x": 0.5}
    counter = Counter(history)
    total = len(history)
    prob_tai = counter["t"] / total
    prob_xiu = counter["x"] / total
    return {"t": prob_tai, "x": prob_xiu}


def apply_probability_threshold(
    prob_dict: Dict[str, float], threshold_t: float = 0.55, threshold_x: float = 0.45
) -> Optional[str]:
    """Applies probability thresholds to make a prediction."""
    if prob_dict["t"] > threshold_t:
        return "t"
    if prob_dict["x"] > threshold_x:
        return "x"
    return None

def statistical_prediction(history: List[str], bias: float = 0.5) -> str:
    """Makes a statistical prediction based on history."""
    if not history:
        return random.choice(["t", "x"])
    counter = Counter(history)
    total = len(history)
    if total == 0:
        return random.choice(["t", "x"])
    prob_tai = counter["t"] / total
    prob_xiu = counter["x"] / total
    if random.random() < prob_tai * (1 + bias) / 2:
        return "t"
    if random.random() < prob_xiu * (1 + bias) / 2 :
        return "x"
    return random.choice(["t", "x"])


def prepare_data_for_models(history: List[str]) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
    """Prepares data for machine learning models."""
    if len(history) < 10:
        return None, None
    if len(set(history[-10:])) < 2:
        return None, None
    try:
        encoded_history = le.fit_transform(history[-10:])
    except ValueError:
        return None, None
    features = np.array([encoded_history], dtype=np.float64)
    features_poly = poly.fit_transform(features)
    X = scaler.fit_transform(features_poly)
    labels = le.transform([history[-1]])
    y = np.array(labels, dtype=np.int64)
    return X, y


def train_all_models() -> None:
    """Trains all machine learning models."""
    if len(train_data) < 10:
        return
    X, Y = [], []
    for history in train_data:
        features, label = prepare_data_for_models(history)
        if features is not None and label is not None:
            X.append(features[0])
            Y.append(label[0])
    if len(X) > 1 and len(Y) > 1:
        X = np.array(X)
        Y = np.array(Y)
        for model in models_to_calibrate:
            try:
                if len(set(Y)) > 1:
                    model.fit(X, Y)
                    calibrated_models[model] = model
                else:
                    print(
                        f"Model {model} skipped. Not enough classes data:{set(Y)} for training."
                    )
            except ValueError as ve:
                print(f"Model {model} error during training: {ve}")
                continue
        model_svm.fit(X, Y)
        try:
            if len(set(Y)) > 1:
                model_calibrated_svm.fit(X, Y)
            else:
                print(
                    f"Model Calibration skipped due to single class data:{set(Y)} for training"
                )
        except ValueError as ve:
            print(f"Model Calibration error: {ve} skip model training")


def ml_prediction(history: List[str]) -> str:
    """Makes a prediction using machine learning models."""
    if len(train_data) < 10:
        return statistical_prediction(history)
    features, label = prepare_data_for_models(history)
    if features is None:
        return statistical_prediction(history)
    try :
      model_svm_prob = model_calibrated_svm.predict_proba(features)
      svm_prediction_label = model_calibrated_svm.predict(features)
    except Exception as e :
         print (f"SVM prediction Model Exception  {e}, fallback Statistical method...")
         return statistical_prediction(history)
    log_prob, log_label = _predict_probabilty(
        calibrated_models.get(model_logistic, model_logistic), features
    )
    sgd_prob, sgd_label = _predict_probabilty(
        calibrated_models.get(model_sgd, model_sgd), features
    )
    rf_prob, rf_label = _predict_probabilty(
        calibrated_models.get(model_rf, model_rf), features
    )
    gb_prob, gb_label = _predict_probabilty(
        calibrated_models.get(model_gb, model_gb), features
    )
    tai_probabilities_average = []
    xiu_probabilities_average = []
    if not np.isnan(log_prob["t"]): tai_probabilities_average.append(log_prob["t"])
    if not np.isnan(sgd_prob["t"]): tai_probabilities_average.append(sgd_prob["t"])
    if not np.isnan(rf_prob["t"]): tai_probabilities_average.append(rf_prob["t"])
    if not np.isnan(gb_prob["t"]): tai_probabilities_average.append(gb_prob["t"])
    if not np.isnan(log_prob["x"]): xiu_probabilities_average.append(log_prob["x"])
    if not np.isnan(sgd_prob["x"]): xiu_probabilities_average.append(sgd_prob["x"])
    if not np.isnan(rf_prob["x"]): xiu_probabilities_average.append(rf_prob["x"])
    if not np.isnan(gb_prob["x"]): xiu_probabilities_average.append(gb_prob["x"])
    average_prob_t = np.mean(tai_probabilities_average) if tai_probabilities_average else 0
    average_prob_x = np.mean(xiu_probabilities_average) if xiu_probabilities_average else 0
    avg_probabilty = {"t": average_prob_t, "x": average_prob_x}
    svm_label = le.inverse_transform(svm_prediction_label)[0]
    predicted_outcome = apply_probability_threshold(avg_probabilty, 0.53, 0.47)
    if predicted_outcome:
        return predicted_outcome
    return svm_label


def _predict_probabilty(model: Any, features: np.ndarray) -> Tuple[Dict[str, float], Optional[str]]:
    """Predicts probabilities and labels for a given model."""
    if hasattr(model, "predict_proba"):
        try:
            probs = model.predict_proba(features)[0]
            labels = le.inverse_transform(model.predict(features))
            prob_dictionary = dict(zip(le.classes_, probs))
            return prob_dictionary, labels[0]
        except ValueError as ve:
            print(f"Model issue with probability: {ve} for {model}")
            return {"t": float("NaN"), "x": float("NaN")}, None
    return {"t": float("NaN"), "x": float("NaN")}, None


def cluster_analysis(history: List[str]) -> Optional[str]:
    """Performs cluster analysis to make a prediction."""
    if len(history) < 5:
        return None
    encoded_history = le.fit_transform(history)
    reshaped_history = encoded_history.reshape(-1, 1)
    try:
        model_kmeans.fit(reshaped_history)
    except ValueError:
        return None
    last_five = le.transform(history[-5:])
    last_five = last_five.reshape(1, -1)
    cluster_label = model_kmeans.predict(last_five[0].reshape(-1, 1))[0]
    counter = Counter(history[-5:])
    if cluster_label == 0:
        return "t" if counter["t"] > counter["x"] else "x"
    if history[-1] == "t":
        return "x"
    return "t"


def analyze_real_data(history: List[str]) -> Optional[str]:
    """Analyzes real data for patterns to make a prediction."""
    if len(history) < 3:
        return None
    if all(item == history[0] for item in history):
        return history[0]
    if all(history[i] != history[i + 1] for i in range(len(history) - 1)):
        return "t" if history[-1] == "x" else "x"
    return None


def deterministic_algorithm(history: List[str]) -> Optional[str]:
    """Applies a deterministic algorithm to make a prediction."""
    if len(history) < 4:
        return None
    if history[-1] == history[-2] == history[-3] and history[-1] == "t":
        return "x"
    if history[-1] == history[-2] == history[-3] and history[-1] == "x":
        return "t"
    if (
        history[-1] != history[-2]
        and history[-2] != history[-3]
        and history[-3] != history[-4]
    ):
        return "t" if history[-1] == "x" else "x"
    return None

def adjust_strategy_weights(feedback: str, strategy: str) -> Dict[str, float]:
    """Adjusts strategy weights based on user feedback."""
    global strategy_weights
    weight_change = feedback_weights.get(feedback, 0.0)
    strategy_weights[strategy] += weight_change * strategy_weights[strategy] * 0.15
    strategy_weights[strategy] = min(max(strategy_weights[strategy], 0.01), 2.0)
    return strategy_weights


def mathematical_calculation(history: List[str]) -> Optional[str]:
    """Performs a mathematical calculation to make a prediction."""
    if not history or len(history) < 5:
        return None
    sequence = "".join(history[-5:])
    t_count = sequence.count("t")
    x_count = sequence.count("x")
    if abs(t_count - x_count) > 2:
        return "x" if t_count > x_count else "t"
    if random.random() < 0.6:
      return "t"
    if random.random() > 0.3:
      return "x"
    return None

def statistical_algorithm(history: List[str]) -> Optional[str]:
    """Applies a statistical algorithm to make a prediction."""
    if not history or len(history) < 10:
        return None
    last_10 = history[-10:]
    count_t = last_10.count("t")
    count_x = last_10.count("x")
    if count_t == count_x:
        return "t" if random.random() < 0.5 else "x"
    diff = abs(count_t - count_x)
    if diff >= 3:
        return "t" if count_t < count_x else "x"
    if random.random() < 0.55:
      return "t"
    if random.random() > 0.45:
       return "x"
    return None

def statistical_analysis(history: List[str]) -> Optional[str]:
    """Performs a statistical analysis to make a prediction."""
    if not history or len(history) < 8:
        return None
    last_8 = history[-8:]
    counter = Counter(last_8)
    total = len(last_8)
    prob_t = counter["t"] / total if "t" in counter else 0
    prob_x = counter["x"] / total if "x" in counter else 0
    if abs(prob_t - prob_x) > 0.3:
        return "t" if prob_x > prob_t else "x"
    if random.random() < 0.5:
       return "t"
    if random.random() > 0.40:
      return "x"
    return None


def numerical_analysis(history: List[str]) -> Optional[str]:
    """Performs a numerical analysis to make a prediction."""
    if not history or len(history) < 6:
        return None
    numeric_representation = [1 if h == "t" else 0 for h in history[-6:]]
    sum_val = sum(numeric_representation)
    average_val = sum_val / len(numeric_representation)
    if average_val >= 0.70:
        return "t" if random.random() < 0.6 else "x"
    if average_val < 0.30:
        return "x" if random.random() > 0.4 else "t"
    return None


def search_algorithm(history: List[str]) -> Optional[str]:
    """Applies a search algorithm to make a prediction."""
    if not history or len(history) < 7:
        return None
    sequence_last_7 = "".join(history[-7:])
    patterns = ["ttx", "xtt", "txt", "xxt", "xtx", "txx"]
    for pattern in patterns:
        if pattern in sequence_last_7:
            return "x" if pattern[0] == "t" else "t"
    return None


def automatic_programming(history: List[str]) -> Optional[str]:
    """Applies an automatic programming logic to make a prediction."""
    if not history or len(history) < 5:
        return None
    last_5 = "".join(history[-5:])
    if last_5 in ["ttttt", "xxxxx"]:
        return "x" if last_5[0] == "t" else "t"
    if last_5 in ["xtxtx", "txtxt", "txxtx", "xttxx"]:
        return "t" if last_5[0] == "x" else "x"
    return None


def theorem_algorithm(history: List[str]) -> Optional[str]:
    """Applies a theorem based algorithm to make a prediction."""
    if not history or len(history) < 6:
        return None
    last_6 = history[-6:]
    t_count = last_6.count("t")
    x_count = last_6.count("x")
    if (t_count == 3 and x_count == 3) or (t_count == 4 and x_count == 2) or (t_count == 2 and x_count == 4):
      return "t" if random.random() < 0.5 else "x"
    if t_count > x_count:
      return "x"
    if x_count > t_count:
      return "t"
    return None

def evolutionary_algorithm(history: List[str]) -> Optional[str]:
    """Applies an evolutionary algorithm to make a prediction."""
    if not history or len(history) < 10:
        return None
    sequence = "".join(history[-10:])
    if "ttttt" in sequence or "xxxxx" in sequence:
        return "x" if sequence[0] == "t" else "t"
    t_counts = [sequence.count(f't{"t" * i}') for i in range(1, 3)]
    x_counts = [sequence.count(f'x{"x" * i}') for i in range(1, 3)]
    if sum(t_counts) > sum(x_counts):
        return "x" if random.random() > 0.2 else None
    if sum(x_counts) > sum(t_counts):
        return "t" if random.random() > 0.2 else None
    return None


def reading_opportunity(history: List[str]) -> Optional[str]:
    """Applies an reading opportunity algorithm to make a prediction."""
    if not history or len(history) < 5:
        return None
    last_5_seq = "".join(history[-5:])
    if last_5_seq.count("t") == 3 and last_5_seq.count("x") == 2:
        return "x" if random.random() < 0.6 else "t"
    if last_5_seq.count("x") == 3 and last_5_seq.count("t") == 2:
        return "t" if random.random() < 0.6 else "x"
    if "xtxt" in last_5_seq or "txtx" in last_5_seq:
      if random.random() > 0.4:
         return "t"
      if random.random() > 0.2:
         return "x"
    return None

def combined_prediction(history: List[str]) -> str:
    """Combines all prediction methods to make a final prediction."""
    global last_prediction
    strategy = None
    algo_prediction = deterministic_algorithm(history)
    if algo_prediction:
        strategy = "deterministic"
        last_prediction.update({"strategy": strategy, "result": algo_prediction})
        return algo_prediction
    cluster_prediction = cluster_analysis(history)
    if cluster_prediction:
        strategy = "cluster"
        last_prediction.update({"strategy": strategy, "result": cluster_prediction})
        return cluster_prediction
    ml_pred = ml_prediction(history)
    if ml_pred:
        strategy = "machine_learning"
        last_prediction.update({"strategy": strategy, "result": ml_pred})
        return ml_pred
    probability_dict = calculate_probabilities(history)
    probability_pred = apply_probability_threshold(probability_dict)
    if probability_pred:
        strategy = "probability"
        last_prediction.update({"strategy": strategy, "result": probability_pred})
        return probability_pred
    streak_prediction = analyze_real_data(history)
    if streak_prediction:
        strategy = "streak"
        last_prediction.update({"strategy": strategy, "result": streak_prediction})
        return streak_prediction
    math_prediction = mathematical_calculation(history)
    if math_prediction:
        strategy = "mathematical"
        last_prediction.update({"strategy": strategy, "result": math_prediction})
        return math_prediction
    statistical_algo_pred = statistical_algorithm(history)
    if statistical_algo_pred:
        strategy = "statistical_algo"
        last_prediction.update(
            {"strategy": strategy, "result": statistical_algo_pred}
        )
        return statistical_algo_pred
    statistical_analysis_pred = statistical_analysis(history)
    if statistical_analysis_pred:
        strategy = "statistical_analysis"
        last_prediction.update(
            {"strategy": strategy, "result": statistical_analysis_pred}
        )
        return statistical_analysis_pred
    numerical_analysis_pred = numerical_analysis(history)
    if numerical_analysis_pred:
        strategy = "numerical_analysis"
        last_prediction.update(
            {"strategy": strategy, "result": numerical_analysis_pred}
        )
        return numerical_analysis_pred
    search_algo_pred = search_algorithm(history)
    if search_algo_pred:
        strategy = "search_algorithm"
        last_prediction.update({"strategy": strategy, "result": search_algo_pred})
        return search_algo_pred
    automatic_prog_pred = automatic_programming(history)
    if automatic_prog_pred:
        strategy = "automatic_program"
        last_prediction.update({"strategy": strategy, "result": automatic_prog_pred})
        return automatic_prog_pred
    theorem_algo_pred = theorem_algorithm(history)
    if theorem_algo_pred:
        strategy = "theorem_algo"
        last_prediction.update({"strategy": strategy, "result": theorem_algo_pred})
        return theorem_algo_pred
    evolutionary_algo_pred = evolutionary_algorithm(history)
    if evolutionary_algo_pred:
        strategy = "evolutionary_algo"
        last_prediction.update(
            {"strategy": strategy, "result": evolutionary_algo_pred}
        )
        return evolutionary_algo_pred
    reading_opportunity_pred = reading_opportunity(history)
    if reading_opportunity_pred:
        strategy = "reading_opportunity"
        last_prediction.update(
            {"strategy": strategy, "result": reading_opportunity_pred}
        )
        return reading_opportunity_pred
    strategy = "boosting"
    last_prediction.update(
        {"strategy": strategy, "result": statistical_prediction(history, 0.3)}
    )
    return statistical_prediction(history, 0.3)


# --- Training Status Calculation ---
def calculate_training_status() -> Dict[str, Any]:
    """Calculates the training status of the bot."""
    total_predictions = len(user_feedback_history)
    if total_predictions == 0:
        return {"status": "ü§ñ Ch∆∞a ƒë·ªß d·ªØ li·ªáu.", "accuracy": 0, "intelligence": 0}
    correct_predictions = sum(
        1 for fb in user_feedback_history if fb["feedback"] == "correct"
    )
    accuracy_percentage = (
        correct_predictions / total_predictions
    ) * 100 if total_predictions > 0 else 0
    intelligence_level = (
        np.mean(list(strategy_weights.values())) * 25 if strategy_weights else 0
    )
    status_report = {
        "status": "üí™ Bot ƒëang ƒë∆∞·ª£c hu·∫•n luy·ªán.",
        "accuracy": accuracy_percentage,
        "intelligence": intelligence_level if intelligence_level <= 100 else 100,
    }
    return status_report


# --- Telegram Bot Command Handlers ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /start command."""
    start_text = (
        "‚ú® Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi *{BOT_NAME}*!\n\n"
        "üé≤ S·ª≠ d·ª•ng /tx [d√£y l·ªãch s·ª≠] ƒë·ªÉ nh·∫≠n d·ª± ƒëo√°n T√†i/X·ªâu.\n"
        "‚ûï S·ª≠ d·ª•ng /add [k·∫øt qu·∫£] ƒë·ªÉ th√™m k·∫øt qu·∫£ th·ª±c t·∫ø.\n"
        "üîÑ S·ª≠ d·ª•ng /update ƒë·ªÉ c·∫≠p nh·∫≠t to√†n b·ªô l·ªãch s·ª≠ c∆∞·ª£c.\n"
        "üíæ S·ª≠ d·ª•ng /save ƒë·ªÉ l∆∞u d·ªØ li·ªáu hi·ªán t·∫°i.\n"
        "üìú Nh·∫≠p /history ƒë·ªÉ xem l·ªãch s·ª≠ c∆∞·ª£c g·∫ßn nh·∫•t.\n"
        "üìä Nh·∫≠p /chart ƒë·ªÉ xem bi·ªÉu ƒë·ªì t·∫ßn su·∫•t.\n"
        "üíæ Nh·∫≠p /logchart ƒë·ªÉ l∆∞u bi·ªÉu ƒë·ªì hi·ªán t·∫°i v√†o server.\n"
        "üßê Nh·∫≠p /status ƒë·ªÉ xem tr·∫°ng th√°i v√† ƒë·ªô th√¥ng minh bot\n\n"
        "B·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng b·∫±ng c√°ch nh·∫≠p c√°c l·ªánh tr√™n, ƒë·ªÉ tr·∫£i nghi·ªám!\n"
    )
    await update.message.reply_text(
        start_text.format(BOT_NAME=BOT_NAME), parse_mode=ParseMode.MARKDOWN
    )


async def update_cau(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /update command."""
    try:
        save_cau_data()
        await update.message.reply_text("üîÑ ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu c·∫ßu v√†o file.")
    except Exception as e:
        print(f"Could not save cau data during update command: {e}")
        await update.message.reply_text("üîÑ Kh√¥ng th·ªÉ c·∫≠p nh·∫≠t d·ªØ li·ªáu c·∫ßu.")


async def save_bot_data(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /save command."""
    try:
        save_data_state()
        await update.message.reply_text("üíæ ƒê√£ l∆∞u d·ªØ li·ªáu bot v√†o file.")
    except Exception as e:
        print(f"Error during saving data state: {e}")
        await update.message.reply_text("üíæ Kh√¥ng th·ªÉ l∆∞u d·ªØ li·ªáu.")


async def tx(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /tx command."""
    try:
        user_input = " ".join(context.args)
        if not user_input:
            await update.message.reply_text(
                "üìù Vui l√≤ng nh·∫≠p d√£y l·ªãch s·ª≠ (t: T√†i, x: X·ªâu)."
            )
            return
        history = user_input.split()
        for item in history:
           if item not in ["t", "x"]:
               await update.message.reply_text(
                 "üö´ D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá. L·ªãch s·ª≠ ch·ªâ ch·ª©a 't' (T√†i) ho·∫∑c 'x' (X·ªâu)."
               )
               return
        history_data.extend(history)
        if len(history) >= 5:
             train_data.append(list(history_data))
             train_labels.append(history[-1])
        train_all_models()
        result = combined_prediction(list(history_data))
        last_prediction["model"] = BOT_NAME
        keyboard = [
            [InlineKeyboardButton("‚úÖ ƒê√∫ng", callback_data="correct")],
            [InlineKeyboardButton("‚ùå Sai", callback_data="incorrect")],
        ]
        reply_markup = InlineKeyboardMarkup(keyboard)
        formatted_result = f"üîÆ K·∫øt qu·∫£ d·ª± ƒëo√°n t·ª´ *{BOT_NAME}*: *{'‚ú®T√†i‚ú®' if result == 't' else 'üñ§X·ªâuüñ§'}* "
        await update.message.reply_text(
            formatted_result, reply_markup=reply_markup, parse_mode=ParseMode.MARKDOWN
        )
    except Exception as e:
        await update.message.reply_text(f"‚ö†Ô∏è L·ªói: {e}")


async def add(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /add command."""
    try:
        user_input = " ".join(context.args)
        if not user_input:
            await update.message.reply_text(
                "üìù Vui l√≤ng nh·∫≠p k·∫øt qu·∫£ th·ª±c t·∫ø (t: T√†i, x: X·ªâu)!"
            )
            return
        new_data = user_input.split()
        for item in new_data:
            if item not in ["t", "x"]:
                await update.message.reply_text(
                    "üö´ D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá. K·∫øt qu·∫£ ch·ªâ ch·ª©a 't' (T√†i) ho·∫∑c 'x' (X·ªâu)."
                )
                return
        history_data.extend(new_data)
        for i in range(len(new_data)):
           if len(history_data) >= 5 + i :
            train_data.append(list(history_data[:len(history_data)-i]) )
            train_labels.append(new_data[i] if i < len(new_data) else new_data[-1])
        train_all_models()
        await update.message.reply_text(f"‚ûï ƒê√£ c·∫≠p nh·∫≠t d·ªØ li·ªáu: {new_data}")
    except Exception as e:
        await update.message.reply_text(f"‚ö†Ô∏è L·ªói: {e}")


async def button(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles button press events."""
    query = update.callback_query
    await query.answer()
    feedback = query.data
    global user_feedback_history
    if (
        last_prediction.get("strategy") is None
        or last_prediction.get("result") is None
        or last_prediction.get("model") is None
    ):
        await query.edit_message_text(
            "‚ö†Ô∏è Kh√¥ng th·ªÉ ghi nh·∫≠n ph·∫£n h·ªìi. Vui l√≤ng th·ª≠ l·∫°i sau."
        )
        return
    if feedback == "correct":
        user_feedback_history.append(
            {
                "result": last_prediction["result"],
                "strategy": last_prediction["strategy"],
                "feedback": "correct",
                "timestamp": time.time(),
            }
        )
        save_user_feedback("correct")
        await query.edit_message_text("‚úÖ C·∫£m ∆°n! Ph·∫£n h·ªìi ƒë√£ ƒë∆∞·ª£c ghi nh·∫≠n.")
    elif feedback == "incorrect":
        user_feedback_history.append(
            {
                "result": last_prediction["result"],
                "strategy": last_prediction["strategy"],
                "feedback": "incorrect",
                "timestamp": time.time(),
            }
        )
        save_user_feedback("incorrect")
        await query.edit_message_text("‚ùå C·∫£m ∆°n! T√¥i s·∫Ω c·ªë g·∫Øng c·∫£i thi·ªán.")
    adjust_strategy_weights(feedback, last_prediction["strategy"])


async def history(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /history command."""
    if not history_data:
        await update.message.reply_text("üìú Ch∆∞a c√≥ d·ªØ li·ªáu l·ªãch s·ª≠.")
    else:
        await update.message.reply_text(f"üìú L·ªãch s·ª≠ g·∫ßn ƒë√¢y: {' '.join(history_data)}")


async def chart(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /chart command."""
    chart_image = generate_history_chart(history_data)
    if chart_image is None:
        await update.message.reply_text("üìä Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì.")
        return
    try:
        await update.message.reply_photo(
            photo=chart_image, caption="üìà Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t k·∫øt qu·∫£."
        )
    except Exception as e:
        print(f"Error sending chart: {e}")
        await update.message.reply_text("üìä Kh√¥ng th·ªÉ t·∫°o bi·ªÉu ƒë·ªì. L·ªói kh√¥ng x√°c ƒë·ªãnh")


def save_current_history_image() -> None:
    """Saves the current history chart to an image file."""
    chart_image = generate_history_chart(history_data)
    if chart_image:
        try:
            with open("history_chart.png", "wb") as f:
                f.write(chart_image.getvalue())
            print("Chart image saved to ./history_chart.png")
        except Exception as e:
            print(f"Error saving chart image: {e}")


async def logchart(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /logchart command."""
    try:
        save_current_history_image()
        await update.message.reply_text("üíæ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o m√°y ch·ªß.")
    except Exception as e:
        print(f"Could not log chart: {e}")
        await update.message.reply_text("üíæ Kh√¥ng th·ªÉ l∆∞u bi·ªÉu ƒë·ªì.")


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /help command."""
    help_text = (
        f"‚ú® H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng *{BOT_NAME}*:\n\n"
        f"   üé≤ /tx [d√£y l·ªãch s·ª≠]: Nh·∫≠n d·ª± ƒëo√°n k·∫øt qu·∫£ T√†i/X·ªâu.\n"
        f"   ‚ûï /add [k·∫øt qu·∫£]: C·∫≠p nh·∫≠t k·∫øt qu·∫£ th·ª±c t·∫ø.\n"
        f"   üîÑ /update: C·∫≠p nh·∫≠t to√†n b·ªô l·ªãch s·ª≠ c∆∞·ª£c v√†o file.\n"
        f"  üíæ  /save: L∆∞u d·ªØ li·ªáu bot hi·ªán t·∫°i v√†o file\n"
        f"   üìú /history : Xem l·ªãch s·ª≠ g·∫ßn ƒë√¢y.\n"
        f"   üìä /chart : Xem bi·ªÉu ƒë·ªì t·∫ßn su·∫•t.\n"
        f"   üíæ /logchart : L∆∞u bi·ªÉu ƒë·ªì v√†o m√°y ch·ªß.\n"
        f"   üßê /status : Xem tr·∫°ng th√°i hu·∫•n luy·ªán v√† ƒë·ªô ch√≠nh x√°c c·ªßa bot.\n\n"
        f"     _V√≠ d·ª•:_\n"
        f"         - /tx t t x t x\n"
        f"         - /add t x x t t"
    )
    await update.message.reply_text(
        help_text.format(BOT_NAME=BOT_NAME), parse_mode=ParseMode.MARKDOWN
    )

async def status(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handles the /status command."""
    training_report = calculate_training_status()
    formatted_message = (
        f"ü§ñ Tr·∫°ng th√°i *{BOT_NAME}*:\n\n"
        f"   üìä T√¨nh tr·∫°ng: {training_report['status']}\n"
        f"   ‚úÖ ƒê·ªô ch√≠nh x√°c: *{training_report['accuracy']:.2f}%*\n"
        f"   üß† M·ª©c ƒë·ªô th√¥ng minh: *{training_report['intelligence']:.2f}/100*\n"
    )
    await update.message.reply_text(formatted_message, parse_mode=ParseMode.MARKDOWN)


if __name__ == "__main__":
    create_feedback_table()
    load_data_state()
    load_cau_data()
    app = ApplicationBuilder().token(TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("tx", tx))
    app.add_handler(CommandHandler("add", add))
    app.add_handler(CommandHandler("history", history))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("chart", chart))
    app.add_handler(CommandHandler("logchart", logchart))
    app.add_handler(CommandHandler("status", status))
    app.add_handler(CommandHandler("update", update_cau))
    app.add_handler(CommandHandler("save", save_bot_data))
    app.add_handler(CallbackQueryHandler(button))
    print("Bot is running...")
    app.run_polling()
    print("Bot has stopped.")